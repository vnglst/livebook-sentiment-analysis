<!-- livebook:{"persist_outputs":true} -->

# Sentiment Analysis toots Jose Valim

```elixir
Mix.install(
  [
    {:httpoison, ">= 0.0.0"},
    {:jason, ">= 0.0.0"},
    {:kino_bumblebee, "~> 0.1.0"},
    {:exla, "~> 0.4.1"},
    {:kino_vega_lite, "~> 0.1.7"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)

alias VegaLite, as: Vl
```

<!-- livebook:{"output":true} -->

```
VegaLite
```

## Fetching statuses

```elixir
%{body: body} = HTTPoison.get!("https://maakr.social/api/v1/accounts/109434491871265383/statuses")
```

<!-- livebook:{"output":true} -->

```
%HTTPoison.Response{
  status_code: 200,
  body: "[{\"id\":\"109484294107511553\",\"created_at\":\"2022-12-09T14:55:46.295Z\",\"in_reply_to_id\":null,\"in_reply_to_account_id\":null,\"sensitive\":false,\"spoiler_text\":\"\",\"visibility\":\"public\",\"language\":null,\"uri\":\"https://genserver.social/objects/2df9e33e-31ab-4f92-8a26-834cd1f5ff7d\",\"url\":\"https://genserver.social/objects/2df9e33e-31ab-4f92-8a26-834cd1f5ff7d\",\"replies_count\":0,\"reblogs_count\":0,\"favourites_count\":0,\"edited_at\":null,\"content\":\"Two years ago I would never have dreamed that I would be writing a guest article on Hugging Face's blog talking about the development of Machine Learning in Elixir: \\u003ca href=\\\"https://huggingface.co/blog/elixir-bumblebee\\\" rel=\\\"nofollow noopener noreferrer\\\" target=\\\"_blank\\\"\\u003ehttps://huggingface.co/blog/elixir-bumblebee\\u003c/a\\u003e\\u003cbr\\u003e\\u003cbr\\u003eWe've made great progress and more to come! üöÄ\",\"reblog\":null,\"account\":{\"id\":\"109434491871265383\",\"username\":\"josevalim\",\"acct\":\"josevalim@genserver.social\",\"display_name\":\"Jos√© Valim\",\"locked\":false,\"bot\":false,\"discoverable\":true,\"group\":false,\"created_at\":\"2022-11-30T00:00:00.000Z\",\"note\":\"Creator of @elixirlang. Livestreams at \\u003ca href=\\\"http://twitch.tv/josevalim\\\" rel=\\\"nofollow noopener noreferrer\\\" target=\\\"_blank\\\"\\u003ehttp://twitch.tv/josevalim\\u003c/a\\u003e.\",\"url\":\"https://genserver.social/users/josevalim\",\"avatar\":\"https://s3-eu-central-1.amazonaws.com/maakr-social/cache/accounts/avatars/109/434/491/871/265/383/original/c0bd7b4312afd53e.jpg\",\"avatar_static\":\"https://s3-eu-central-1.amazonaws.com/maakr-social/cache/accounts/avatars/109/434/491/871/265/383/original/c0bd7b4312afd53e.jpg\",\"header\":\"https://maakr.social/headers/original/missing.png\",\"header_static\":\"https://maakr.social/headers/original/missing.png\",\"followers_count\":563,\"following_count\":9,\"statuses_count\":24,\"last_status_at\":\"2022-12-09\",\"emojis\":[],\"fields\":[]},\"media_attachments\":[],\"mentions\":[],\"tags\":[],\"emojis\":[],\"card\":{\"url\":\"https://huggingface.co/blog/elixir-bumblebee\",\"title\":\"From GPT2 to Stable Diffusion: Hugging Face arrives to the Elixir community\",\"description\":\"We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.\",\"type\":\"link\",\"author_name\":\"\",\"author_url\":\"\",\"provider_name\":\"\",\"provider_url\":\"\",\"html\":\"\",\"width\":400,\"height\":200,\"image\":\"https://s3-eu-central-1.amazonaws.com/maakr-social/cache/preview_cards/images/000/014/619/original/7bcc518c574e540d.png\",\"embed_url\":\"\",\"blurhash\":\"UROo0v-hJC?O~JXLa{jYJ[%DxBM.$wR*WXkC\"},\"poll\":null},{\"id\":\"109480171145649188\",\"created_at\":\"2022-12-08T21:27:14.577Z\",\"in_reply_to_id\":null,\"in_reply_to_account_id\":null,\"sensitive\":false,\"spoiler_text\":\"\",\"visibility\":\"public\",\"language\":null,\"uri\":\"https://genserver.social/objects/0304ef84-3005-4c9c-a59e-e4d840627247\",\"url\":\"https://genserver.social/objects/0304ef84-3005-4c9c-a59e-e4d840627247\",\"replies_count\":0,\"reblogs_count\":0,\"favourites_count\":0,\"edited_at\":null,\"content\":\"Heart is beating fast. This is such a weird rush. :D\",\"reblog\":null,\"account\":{\"id\":\"109434491871265383\",\"username\":\"josevalim\",\"acct\":\"josevalim@genserver.social\",\"display_name\":\"Jos√© Valim\",\"locked\":false,\"bot\":false,\"discoverable\":true,\"group\":false,\"created_at\":\"2022-11-30T00:00:00.000Z\",\"note\":\"Creator of @elixirlang. Livestreams at \\u003ca href=\\\"http://twitch.tv/josevalim\\\" rel=\\\"nofollow noopener noreferrer\\\" target=\\\"_blank\\\"\\u003ehttp://twitch.tv/josevalim\\u003c/a\\u003e.\",\"url\":\"https://genserver.social/users/josevalim\",\"avatar\":\"https://s3-eu-central-1.amazonaws.com/maakr-social/cache/accounts/avatars/109/434/491/871/265/383/original/c0bd7b4312afd53e.jpg\",\"avatar_static\":\"https://s3-eu-central-1.amazonaws.com/maakr-social/cache/accounts/avatars/109/434/491/871/265/383/original/c0bd7b4312afd53e.jpg\",\"header\":\"https://maakr.social/headers/original/missing.png\",\"header_static\":\"https://maakr.social/headers/original/missing.png\",\"followers_count\":563,\"following_count\":9,\"statuses_count\":24,\"last_status_at\":\"2022-12-09\",\"emojis\":[],\"fields\":[]},\"media_attachments\":[],\"mentions\":[],\"tags\":[],\"emojis\":[],\"card\":nul" <> ...,
  headers: [
    {"Date", "Fri, 09 Dec 2022 19:39:43 GMT"},
    {"Content-Type", "application/json; charset=utf-8"},
    {"Transfer-Encoding", "chunked"},
    {"Connection", "keep-alive"},
    {"Vary", "Accept-Encoding"},
    {"Server", "Mastodon"},
    {"X-Frame-Options", "DENY"},
    {"X-Content-Type-Options", "nosniff"},
    {"X-XSS-Protection", "0"},
    {"Permissions-Policy", "interest-cohort=()"},
    {"X-RateLimit-Limit", "300"},
    {"X-RateLimit-Remaining", "298"},
    {"X-RateLimit-Reset", "2022-12-09T19:40:00.000963Z"},
    {"Cache-Control", "no-store"},
    {"Link",
     "<https://maakr.social/api/v1/accounts/109434491871265383/statuses?max_id=109444193916466067>; rel=\"next\", <https://maakr.social/api/v1/accounts/109434491871265383/statuses?min_id=109484294107511553>; rel=\"prev\""},
    {"ETag", "W/\"5c47b605298f854b05a37e7b43dd8345\""},
    {"Content-Security-Policy",
     "base-uri 'none'; default-src 'none'; frame-ancestors 'none'; font-src 'self' https://maakr.social; img-src 'self' https: data: blob: https://maakr.social; style-src 'self' https://maakr.social 'nonce-fHDIlHFcT0U9LMkY+ZeUCQ=='; media-src 'self' https: data: https://maakr.social; frame-src 'self' https:; manifest-src 'self' https://maakr.social; connect-src 'self' data: blob: https://maakr.social https://s3-eu-central-1.amazonaws.com wss://maakr.social; script-src 'self' https://maakr.social 'wasm-unsafe-eval'; child-src 'self' blob: https://maakr.social; worker-src 'self' blob: https://maakr.social"},
    {"X-Request-Id", "133c83f2-692a-4692-ad8d-573c5de3123b"},
    {"X-Runtime", "0.113973"},
    {"Strict-Transport-Security", "max-age=63072000; includeSubDomains"},
    {"Vary", "Origin"},
    {"X-Cached", "MISS"},
    {"Strict-Transport-Security", "max-age=31536000"}
  ],
  request_url: "https://maakr.social/api/v1/accounts/109434491871265383/statuses",
  request: %HTTPoison.Request{
    method: :get,
    url: "https://maakr.social/api/v1/accounts/109434491871265383/statuses",
    headers: [],
    body: "",
    params: %{},
    options: []
  }
}
```

## Formatting

```elixir
contents =
  body
  |> Jason.decode!()
  |> Enum.map(fn %{"content" => content} -> content end)
  |> Enum.reject(fn str -> str == "" end)
  |> Enum.map(&String.replace(&1, ~r/<[a-zA-Z\/][^>]*>/, " "))
```

<!-- livebook:{"output":true} -->

```
["Two years ago I would never have dreamed that I would be writing a guest article on Hugging Face's blog talking about the development of Machine Learning in Elixir:  https://huggingface.co/blog/elixir-bumblebee   We've made great progress and more to come! üöÄ",
 "Heart is beating fast. This is such a weird rush. :D",
 "We are eyeing Whisper for sure. The ML model parts should be easy. But we probably need some toolkit for audio processing.",
 "The announcement is on the home page of the orange site, in case you want to help get the word out. :)",
 "This would not be possible without:  * Jonatan K≈Çosko, Sean Moriarity, and Paulo Valente (from @dashbit and @dockyard) * Hugging Face for truly making AI accessible across teams and ecosystems * The Erlang VM (@erlang_org)  Thank you! ‚ù§Ô∏èüß°üíõüíöüíôüíú",
 "You can do all of this within the same Elixir instance (or distributed!). This is HUGE. From Phoenix LiveView to Bumblebee, Elixir + Erlang allows you to drop several layers of complexity and focus on your code. No JSON APIs, no gRPCs, no additional services. Just function calls.  [3/4]",
 "The best part: you can explore Machine Learning models in Livebook and then easily embed them into your Phoenix applications, Broadway pipelines, Nerves devices, etc. This is the beginning of opening the Elixir ecosystem to a whole new category of applications.  [2/4]",
 "Announcing Bumblebee: from GPT2 to Stable Diffusion neural networks for Elixir. And - with Livebook v0.8 - all it takes for you to give it a try is three clicks.  Watch the video announcement:  https://www.youtube.com/watch?v=g3oyh3g1AtQ   Read the news:  https://news.livebook.dev/announcing-bumblebee-gpt2-stable-diffusion-and-more-in-elixir-3Op73O   [1/4]",
 "Bee noises intensifies! zzzzzzzzzzzzzzz",
 "Congratulations to everyone who participated on  https://spawnfest.org/ !  A special thank you to those who used this opportunity to contribute to  https://livebook.dev/ , your work is definitely inspiring the community and the team to continue improving it. ‚ù§Ô∏è",
 "More context for the uninitiated: When deploying machine learning models, especially in the GPU, you want to batch several inputs before doing the work. For example, if you are going to classify an image, you would rather classify 10 of them at once. Nx.Serving abstracts all of this for you.  PS: This is not the HUGE announcement. ETA for that is tomorrow (Thu UTC).  [2/2]",
 "Nx v0.4.1 is out with a new feature called Nx.Serving:  https://hexdocs.pm/nx/Nx.Serving.html   For those familiar with TensorFlow Serving, this achieves the same but within the same Erlang VM, so you don't need 3rd-party dependencies.  This makes it practical to deploy ML models as part of your Phoenix apps, Broadway pipelines, and Nerves systems!  [1/2]",
 "  @ brainlid      @ cocoa    a project that uses elixir_make may opt-in to have a precompiler and then ship prebuilt binaries (so users don't need make+gcc to build the dep in the first place). Here is an example:  https://github.com/elixir-nx/stb_image/pull/22 ",
 "  @ Tuxified    the ants represent misdirection üòÖ",
 "I am very excited because, if all goes according to plan, this week we are going to release something HUGE, never seen before on Elixir or the BEAM!",
 "We have released elixir_make v0.7.0:  hexdocs.pm/elixir_make/   Its main new feature is precompilation support, fully implemented by   @ cocoa   !"]
```

## Loading model

```elixir
{:ok, model_info} =
  Bumblebee.load_model({:hf, "finiteautomata/bertweet-base-sentiment-analysis"},
    log_params_diff: false
  )

{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "vinai/bertweet-base"})

serving =
  Bumblebee.Text.text_classification(model_info, tokenizer,
    compile: [batch_size: 1, sequence_length: 100],
    defn_options: [compiler: EXLA]
  )
```

<!-- livebook:{"output":true} -->

```

20:39:44.832 [info] TfrtCpuClient created.

```

<!-- livebook:{"output":true} -->

```
%Nx.Serving{
  module: Nx.Serving.Default,
  arg: #Function<1.49867295/0 in Bumblebee.Text.TextClassification.text_classification/3>,
  client_preprocessing: #Function<2.49867295/1 in Bumblebee.Text.TextClassification.text_classification/3>,
  client_postprocessing: #Function<3.49867295/3 in Bumblebee.Text.TextClassification.text_classification/3>,
  process_options: [batch_size: 1]
}
```

## Sentiment analysis

```elixir
outputs =
  contents
  |> Enum.map(fn str -> {str, Nx.Serving.run(serving, str)} end)
  |> Enum.map(fn {str, output} ->
    highest = output.predictions |> Enum.at(0)
    %{str: str, label: highest.label, score: highest.score}
  end)
```

<!-- livebook:{"output":true} -->

```
[
  %{
    label: "POS",
    score: 0.9909659624099731,
    str: "Two years ago I would never have dreamed that I would be writing a guest article on Hugging Face's blog talking about the development of Machine Learning in Elixir:  https://huggingface.co/blog/elixir-bumblebee   We've made great progress and more to come! üöÄ"
  },
  %{
    label: "POS",
    score: 0.9739554524421692,
    str: "Heart is beating fast. This is such a weird rush. :D"
  },
  %{
    label: "POS",
    score: 0.6387661695480347,
    str: "We are eyeing Whisper for sure. The ML model parts should be easy. But we probably need some toolkit for audio processing."
  },
  %{
    label: "NEU",
    score: 0.578300416469574,
    str: "The announcement is on the home page of the orange site, in case you want to help get the word out. :)"
  },
  %{
    label: "POS",
    score: 0.985962450504303,
    str: "This would not be possible without:  * Jonatan K≈Çosko, Sean Moriarity, and Paulo Valente (from @dashbit and @dockyard) * Hugging Face for truly making AI accessible across teams and ecosystems * The Erlang VM (@erlang_org)  Thank you! ‚ù§Ô∏èüß°üíõüíöüíôüíú"
  },
  %{
    label: "NEU",
    score: 0.7250223159790039,
    str: "You can do all of this within the same Elixir instance (or distributed!). This is HUGE. From Phoenix LiveView to Bumblebee, Elixir + Erlang allows you to drop several layers of complexity and focus on your code. No JSON APIs, no gRPCs, no additional services. Just function calls.  [3/4]"
  },
  %{
    label: "POS",
    score: 0.9076465368270874,
    str: "The best part: you can explore Machine Learning models in Livebook and then easily embed them into your Phoenix applications, Broadway pipelines, Nerves devices, etc. This is the beginning of opening the Elixir ecosystem to a whole new category of applications.  [2/4]"
  },
  %{
    label: "POS",
    score: 0.6140084266662598,
    str: "Announcing Bumblebee: from GPT2 to Stable Diffusion neural networks for Elixir. And - with Livebook v0.8 - all it takes for you to give it a try is three clicks.  Watch the video announcement:  https://www.youtube.com/watch?v=g3oyh3g1AtQ   Read the news:  https://news.livebook.dev/announcing-bumblebee-gpt2-stable-diffusion-and-more-in-elixir-3Op73O   [1/4]"
  },
  %{label: "NEG", score: 0.9642468094825745, str: "Bee noises intensifies! zzzzzzzzzzzzzzz"},
  %{
    label: "POS",
    score: 0.9924547672271729,
    str: "Congratulations to everyone who participated on  https://spawnfest.org/ !  A special thank you to those who used this opportunity to contribute to  https://livebook.dev/ , your work is definitely inspiring the community and the team to continue improving it. ‚ù§Ô∏è"
  },
  %{
    label: "NEU",
    score: 0.9224608540534973,
    str: "More context for the uninitiated: When deploying machine learning models, especially in the GPU, you want to batch several inputs before doing the work. For example, if you are going to classify an image, you would rather classify 10 of them at once. Nx.Serving abstracts all of this for you.  PS: This is not the HUGE announcement. ETA for that is tomorrow (Thu UTC).  [2/2]"
  },
  %{
    label: "NEU",
    score: 0.5226280093193054,
    str: "Nx v0.4.1 is out with a new feature called Nx.Serving:  https://hexdocs.pm/nx/Nx.Serving.html   For those familiar with TensorFlow Serving, this achieves the same but within the same Erlang VM, so you don't need 3rd-party dependencies.  This makes it practical to deploy ML models as part of your Phoenix apps, Broadway pipelines, and Nerves systems!  [1/2]"
  },
  %{
    label: "NEU",
    score: 0.7518523931503296,
    str: "  @ brainlid      @ cocoa    a project that uses elixir_make may opt-in to have a precompiler and then ship prebuilt binaries (so users don't need make+gcc to build the dep in the first place). Here is an example:  https://github.com/elixir-nx/stb_image/pull/22 "
  },
  %{
    label: "NEG",
    score: 0.8873650431632996,
    str: "  @ Tuxified    the ants represent misdirection üòÖ"
  },
  %{
    label: "POS",
    score: 0.9922645092010498,
    str: "I am very excited because, if all goes according to plan, this week we are going to release something HUGE, never seen before on Elixir or the BEAM!"
  },
  %{
    label: "POS",
    score: 0.936056911945343,
    str: "We have released elixir_make v0.7.0:  hexdocs.pm/elixir_make/   Its main new feature is precompilation support, fully implemented by   @ cocoa   !"
  }
]
```

```elixir
Vl.new(width: 700, height: 400, title: "Sentiment analysis")
|> Vl.data_from_values(outputs, only: ["label", "score"])
|> Vl.mark(:arc)
|> Vl.encode_field(:color, "label", type: :nominal, scale: [scheme: "category10"])
|> Vl.encode_field(:theta, "score", type: :quantitative)
```

<!-- livebook:{"output":true} -->

```vega-lite
{"$schema":"https://vega.github.io/schema/vega-lite/v5.json","data":{"values":[{"label":"POS","score":0.9909659624099731},{"label":"POS","score":0.9739554524421692},{"label":"POS","score":0.6387661695480347},{"label":"NEU","score":0.578300416469574},{"label":"POS","score":0.985962450504303},{"label":"NEU","score":0.7250223159790039},{"label":"POS","score":0.9076465368270874},{"label":"POS","score":0.6140084266662598},{"label":"NEG","score":0.9642468094825745},{"label":"POS","score":0.9924547672271729},{"label":"NEU","score":0.9224608540534973},{"label":"NEU","score":0.5226280093193054},{"label":"NEU","score":0.7518523931503296},{"label":"NEG","score":0.8873650431632996},{"label":"POS","score":0.9922645092010498},{"label":"POS","score":0.936056911945343}]},"encoding":{"color":{"field":"label","scale":{"scheme":"category10"},"type":"nominal"},"theta":{"field":"score","type":"quantitative"}},"height":400,"mark":"arc","title":"Sentiment analysis","width":700}
```
